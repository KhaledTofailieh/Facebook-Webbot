{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><left>- Hello, This Notebook Contains Facebook Web-bot. </left></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "     <ul>\n",
    "    <li><h3><left>Contents:</left></h3></li>\n",
    "    <ul>\n",
    "        <li><a href=\"#import\">Import</a></li>\n",
    "        <li><a href=\"#bot\">Facebook Bot</a></li>\n",
    "        <li><a href=\"#str\">Strategies</a></li>\n",
    "        <li><a href=\"#pipline\">Pipline</a></li>\n",
    "        <li><a href=\"#run\">Running</a></li> \n",
    "        <li><a href=\"#save\">Saving</a></li> \n",
    "    </ul>\n",
    "         </ul>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import\"></a>\n",
    "<li><h3><left>- Import</left></h3>\n",
    "<h5><left>- here we depends on selenium library.</left></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries:\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial values:\n",
    "details_path = 'details.json'\n",
    "website = 'facebook'\n",
    "url = 'https://touch.facebook.com'\n",
    "file_path = '31_8_2021.csv'\n",
    "pages_path = 'pages.csv'\n",
    "data_columns = ['page_id','text','time','reactions','url']\n",
    "pages_columns =['name','id','followers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get user-name and password of facebook account.\n",
    "def get_user_details(file_path,website):\n",
    "    with open(file_path) as jsonfile:\n",
    "        config = json.load(jsonfile)\n",
    "    return config[website]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bot\"></a>\n",
    "<h3><left>- This is abstract Facebook Bot.</left></h3>\n",
    "<h5><left>- contains the basic processes that human can do in facebook, such as searching for page or clicking on post.</left></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FbCrawler:\n",
    "    def __init__(self,browser= None):\n",
    "        if browser is None:\n",
    "            profile = webdriver.FirefoxProfile() \n",
    "            profile.set_preference(\"permissions.default.image\", 2)\n",
    "            # 1 - Allow all images\n",
    "            # 2 - Block all images\n",
    "            # 3 - Block 3rd party images\n",
    "            self.browser = webdriver.Firefox(firefox_profile=profile)\n",
    "        else:\n",
    "            self.browser = browser\n",
    "        self.comments_shares_pattern = r'(\\d+)\\s(\\w+)(\\d+)\\s(\\w+)'\n",
    "        \n",
    "\n",
    "    def start_scraping(self,url):\n",
    "        try:    \n",
    "            self.browser.get(url)\n",
    "        except Exception as ex:\n",
    "            print('some error in getting url!')\n",
    "            \n",
    "    def login_(self,userdetails):\n",
    "        try: \n",
    "            email =  self.browser.find_element_by_id('m_login_email')\n",
    "            password = self.browser.find_element_by_id('m_login_password')\n",
    "            submit = self.browser.find_element_by_class_name('_2pie')\n",
    "\n",
    "            email.send_keys(user_details['user_name'])\n",
    "            WebDriverWait(self.browser, 5)\n",
    "            password.send_keys(user_details['password'])\n",
    "            submit.click()\n",
    "            wait = WebDriverWait(self.browser, 5)\n",
    "        except Exception as ex:\n",
    "            print('some error in logging in!',ex)\n",
    "            \n",
    "    def complete_login(self):\n",
    "        try:\n",
    "            WebDriverWait(self.browser, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, '._52jf')))\n",
    "            search_field = self.browser.find_element_by_css_selector('._52jf')\n",
    "            if search_field is not None and search_field.text in 'تسجيل الدخول بضغطة واحدة':\n",
    "                not_now = self.browser.find_element_by_css_selector('._56bt')\n",
    "                ok = self.browser.find_element_by_css_selector('._56bu')\n",
    "                choise = np.random.randint(2)\n",
    "                if choise is 0:\n",
    "                    ok.click()\n",
    "                else:\n",
    "                    not_now.click()\n",
    "        except Exception as ex:\n",
    "            print(\"some error in complete login or the page is not required!\",ex)\n",
    "            \n",
    "                \n",
    "    def search_on_page(self,page_name):\n",
    "        try:\n",
    "            search_btn = self.browser.find_element_by_id('search_jewel')\n",
    "            search_btn.click()\n",
    "            WebDriverWait(self.browser, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, '#main-search-input')))\n",
    "            search_field = self.browser.find_element_by_css_selector('#main-search-input')\n",
    "            search_field.clear()\n",
    "            search_field.send_keys(page_name)\n",
    "            WebDriverWait(self.browser, 5)\n",
    "            search_field.send_keys(Keys.ENTER)\n",
    "            WebDriverWait(self.browser, 5)\n",
    "        except Exception as ex:\n",
    "            print('some error in searching page!',ex)\n",
    "    \n",
    "    def click_on_pages_tap(self,delay=1):\n",
    "        try: \n",
    "            WebDriverWait(self.browser, delay*5)\n",
    "            pages_tap= self.browser.find_element_by_css_selector('._x0a:nth-child(6)')\n",
    "            pages_tap.click()\n",
    "        except Exception as ex:\n",
    "            print('some error on click page tap!',ex)\n",
    "            \n",
    "    def choose_from_results(self,result_num,delay=1):\n",
    "        try:\n",
    "            WebDriverWait(self.browser, delay*5).until(EC.presence_of_element_located((By.CLASS_NAME, '_2rgt')))\n",
    "            pages = self.browser.find_elements_by_class_name('_2rgt')\n",
    "            pages[result_num].click()\n",
    "        except Exception as ex:\n",
    "            print('some error in choosing from results!',ex)\n",
    "            raise ex\n",
    "            \n",
    "\n",
    "    def get_name(self,delay=1):\n",
    "        name = ''\n",
    "        try: \n",
    "            WebDriverWait(self.browser, delay*2).until(EC.presence_of_element_located((By.CLASS_NAME, '_59k')))\n",
    "            name = self.browser.find_element_by_class_name('_59k').text\n",
    "            print(name)\n",
    "        except Exception as ex:\n",
    "            print('some error in get page name!', ex)\n",
    "        return name\n",
    "        \n",
    "    def get_posts_chunk(self,current):\n",
    "        p = self.browser.find_elements_by_class_name(\"_rnk\")\n",
    "        return p[current:]\n",
    "    \n",
    "    def get_post(self,current):\n",
    "        p = self.browser.find_elements_by_class_name(\"_rnk\")\n",
    "#         p = self.browser.find_elements_by_id(\"u_9_c_jl\")\n",
    "        return p[current]\n",
    "    \n",
    "    def get_post_reactions(self,delay=1):\n",
    "        reactions_list = []\n",
    "        try:\n",
    "            WebDriverWait(self.browser, 5*delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"._1g06\")))\n",
    "            reactions_area =  self.browser.find_element_by_css_selector(\"._1g06\")  \n",
    "            reactions_area.click()    \n",
    "            \n",
    "            WebDriverWait(self.browser, 3*delay).until(EC.presence_of_element_located((By.CLASS_NAME, '_5p-9')))\n",
    "            reactions = self.browser.find_elements_by_class_name('_5p-9')\n",
    "            for r in reactions:\n",
    "                area_label = r.get_attribute('aria-label')\n",
    "                reactions_list.append(area_label)\n",
    "            wait = WebDriverWait(self.browser, 0.5*delay)  \n",
    "            self.browser.execute_script(\"window.history.go(-1)\")\n",
    "        except Exception as ex:\n",
    "            self.browser.execute_script(\"window.history.go(-1)\")\n",
    "            print('some error in get post reactions!',ex)\n",
    "        return reactions_list\n",
    "    \n",
    "    def get_post_text(self,delay=1):\n",
    "        text =[]\n",
    "        try: \n",
    "            WebDriverWait(self.browser, 5*delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'p')))\n",
    "            ps = self.browser.find_elements_by_css_selector('p')\n",
    "            for t in ps:\n",
    "                text.append(t.text)\n",
    "        except Exception as ex:\n",
    "            print('some error in get post text!',ex)\n",
    "            raise ex\n",
    "        return '\\n'.join(text)\n",
    "    \n",
    "    def get_post_date_time(self):\n",
    "        date_time = ''\n",
    "        try:\n",
    "            date_time = self.browser.find_element_by_css_selector('abbr').text\n",
    "        except Exception as ex:\n",
    "            print('some error in get post date-time!',ex)\n",
    "        return date_time\n",
    "    \n",
    "    def get_commentes_shares_count(self):\n",
    "        counts = []\n",
    "        try:\n",
    "            counts = fb_crawler.browser.find_elements_by_css_selector('._1fnt')\n",
    "            re.search(ptn,mm)\n",
    "        except Exception as ex:\n",
    "            print('some error in get post comments!',ex)\n",
    "        return counts \n",
    "    \n",
    "    def return_back(self):\n",
    "        self.browser.execute_script(\"window.history.go(-1)\")\n",
    "        \n",
    "    def scroll_down_by_value(self, value,start=0):\n",
    "        self.browser.execute_script(\"window.scrollBy(\"+str(start)+\",\"+str(value)+\")\")\n",
    "    def scroll_up_by_value(self):\n",
    "        self.browser.execute_script(\"window.scrollTo(document.body.scrollHeight, document.body.scrollHeight-500);\")\n",
    "          \n",
    "    def scroll_down(self,start=0):\n",
    "        self.browser.execute_script(\"window.scrollTo(\"+str(start)+\", document.body.scrollHeight);\")\n",
    "    def scroll_up(self):\n",
    "        self.browser.execute_script(\"window.scrollTo(200, 100);\")\n",
    "          \n",
    "    def page_has_loaded(self):\n",
    "        page_state = self.browser.execute_script('return document.readyState;')\n",
    "        return page_state == 'complete'\n",
    "    def get_current_url(self):\n",
    "        return self.browser.current_url\n",
    "    def clear_memory(self):\n",
    "        self.browser.delete_all_cookies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"str\"></a>\n",
    "<h3><left>- This is Strategies Class.</left></h3>\n",
    "<h5><left>- contains some of algorithms that simulate the human way of surfing Facebook.</left></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FbStartegies:\n",
    "    \n",
    "    def __init__(self,crawler,pipline):\n",
    "        self.crawler = crawler\n",
    "        self.pipline = pipline\n",
    "        \n",
    "    def open_page(self,page_name,delay=1,result_to_select=0):\n",
    "        try:\n",
    "            self.crawler.search_on_page(page_name)\n",
    "            time.sleep(5*delay)\n",
    "           self.crawler.click_on_pages_tap()\n",
    "            page_name = self.crawler.get_name()\n",
    "            page_id= self.pipline.add_to_pages(page_name)\n",
    "            self.crawler.choose_from_results(result_to_select)\n",
    "\n",
    "            print('page choosen: ',result_to_select+1,page_name)\n",
    "            time.sleep(1*delay)\n",
    "        except Exception as ex:\n",
    "            print('some error in open page!',ex)\n",
    "            raise ex\n",
    "            \n",
    "        return page_name\n",
    "   \n",
    "    def reopen_searched_page(self,result_to_select,delay=1):\n",
    "        self.crawler.return_back()\n",
    "        time.sleep(10*delay)\n",
    "        \n",
    "       self.crawler.choose_from_results(result_to_select)\n",
    "        \n",
    "    def get_page_posts(self,page_name,posts_num,start=0,delay=1,result_to_select=0,search_page =True,max_posts_num=60):\n",
    "        crowled_posts = 0\n",
    "        page_id =0\n",
    "        try:\n",
    "            if search_page:\n",
    "                page_id = self.open_page(page_name,result_to_select=result_to_select)\n",
    "            else:\n",
    "                page_id = self.pipline.add_to_pages(page_name)\n",
    "            \n",
    "            try:\n",
    "                i=len(self.crawler.get_posts_chunk(0))\n",
    "                while i < posts_num + start:\n",
    "                    p = self.crawler.get_posts_chunk(i)\n",
    "                    i += len(p)\n",
    "                    self.crawler.scroll_down()\n",
    "                print(i)\n",
    "                \n",
    "            except Exception as ex:\n",
    "                print('some exception in browsing data!',ex)\n",
    "            self.crawler.scroll_up()\n",
    "            WebDriverWait(self.crawler.browser, 5)\n",
    "            time.sleep(3*delay)\n",
    "            i= start\n",
    "            while i < posts_num + start:\n",
    "                try:\n",
    "                    post = {}\n",
    "#                     self.crawler.scroll_up()\n",
    "                    WebDriverWait(self.crawler.browser, 2)\n",
    "                    time.sleep(1*delay)\n",
    "                    p = self.crawler.get_post(i)\n",
    "                    self.crawler.scroll_down_by_value(575)\n",
    "                    p.click()\n",
    "                    \n",
    "                    try:\n",
    "                        post['text'] = self.crawler.get_post_text(delay=delay)\n",
    "                        print(post['text'])\n",
    "                        post['url'] = self.crawler.get_current_url()\n",
    "                        post['time'] = self.crawler.get_post_date_time()\n",
    "                        reactions = self.crawler.get_post_reactions(delay=delay)\n",
    "                        post['reactions'] = ';'.join(reactions)\n",
    "                        post['page_id'] = page_id\n",
    "                        \n",
    "                        self.pipline.add_to_data(post)\n",
    "                        self.crawler.return_back()\n",
    "                    except Exception as ex:\n",
    "                        print('some exception:',ex)\n",
    "                        self.crawler.return_back()\n",
    "\n",
    "                    i+=1\n",
    "                    crowled_posts +=1\n",
    "#                     del post\n",
    "#                     del p \n",
    "                    print(i)\n",
    "                except Exception as ex:\n",
    "                    i+=1\n",
    "                    crowled_posts +=1\n",
    "                    print('some exception:',ex)\n",
    "        except Exception as ex:\n",
    "            print('some error!',ex)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pipline\"></a>\n",
    "<h3><left>- This is Pipline Class.</left></h3>\n",
    "<h5><left>- contains methods to save gained posts into .csv file using pandas.</left></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class Pipeline:\n",
    "    def __init__(self,save_path,pages_path,**kwargs):\n",
    "        columns = None\n",
    "        self.data_path = save_path\n",
    "        if 'data' in kwargs:\n",
    "            self.data = kwargs['data']\n",
    "            \n",
    "        if 'pages' in kwargs:\n",
    "            self.pages = kwargs['pages']\n",
    "        else: \n",
    "            if 'data_columns' in kwargs:\n",
    "                self.data_columns = kwargs['data_columns']\n",
    "                \n",
    "            if 'pages_columns' in kwargs:\n",
    "                self.pages_columns = kwargs['pages_columns']\n",
    "                \n",
    "            self.data = pd.DataFrame(columns=kwargs['data_columns'])\n",
    "            self.pages = pd.DataFrame(columns= kwargs['pages_columns'])\n",
    "                \n",
    "    def add_to_data(self,row):\n",
    "        row = pd.Series(row)\n",
    "        self.data= self.data.append(row, ignore_index=True)\n",
    "    \n",
    "    def add_to_pages(self,page_name):\n",
    "        page_id = 'error_id'\n",
    "        try:\n",
    "            page_id = self.pages.index[self.pages['name']==page_name].tolist() \n",
    "            if len(page_id) == 0:\n",
    "                page_id = int(time.time())\n",
    "                page = pd.Series({'name':page_name,'id':page_id})\n",
    "                self.pages= self.pages.append(page,ignore_index = True)\n",
    "                return page_id\n",
    "            return page_id[0]\n",
    "        except Exception as ex:\n",
    "            print('some error in add page to data!', ex)\n",
    "            \n",
    "    def save_data(self):\n",
    "        try:\n",
    "            self.data.to_csv(self.data_path)\n",
    "        except Exception as ex:\n",
    "            print('some exception saving data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"run\"></a>\n",
    "<h3><left>- Running.</left></h3>\n",
    "<h5><left>- here we create our pipline, run our bot, and finally start gaining posts form facebook.</left></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipline= Pipeline(file_path,pages_path,data_columns = data_columns,pages_columns=pages_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb_crawler = FbCrawler(fb_crawler.browser)\n",
    "fb_crawler = FbCrawler()\n",
    "fb_st = FbStartegies(fb_crawler,pipline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_details = get_user_details(details_path,website)\n",
    "fb_crawler.start_scraping(url=url)\n",
    "fb_crawler.login_(user_details)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the facebook bot need parameters:\n",
    "# page_name, how many post we want it to gain, where do we want it start gaining.\n",
    "page_name = \"يوميات قذيفة هاون في دمشق\"\n",
    "fb_st.get_page_posts(page_name=page_name,posts_num=50,start=0,\n",
    "                     search_page=True,max_posts_num=50,delay =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>reactions</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>لتوليد الكهرباء.. دراسة للاستفادة من غاز الميث...</td>\n",
       "      <td>٧ دقائق</td>\n",
       "      <td></td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>مجلس التعليم العالي: تسوية أوضاع الطلاب المستن...</td>\n",
       "      <td>٣٨ دقيقة</td>\n",
       "      <td>تفاعل ‏٥١٩‏ من الأشخاص مع هذا المنشور;تفاعل ‏٥...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>لباس الرسمي الجديد للمنتخب السوري.. عمر خريبين...</td>\n",
       "      <td>ساعة واحدة</td>\n",
       "      <td>تفاعل ‏٩٢٨‏ من الأشخاص مع هذا المنشور;تفاعل ‏٩...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>مدرسة البشائر العربية الخاصة\\nالتسجيل مستمر لل...</td>\n",
       "      <td>ساعتان</td>\n",
       "      <td>تفاعل ‏٢٦٥‏ من الأشخاص مع هذا المنشور;تفاعل ‏٢...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>تقدير كلفة تنظيف شاطئ بانياس من الفيول بـ3 ملا...</td>\n",
       "      <td>٣ ساعات</td>\n",
       "      <td>تفاعل ‏٢٫٤ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>برأيكن بشو ممكن ترتبط السيارة بيوم السبت بالمت...</td>\n",
       "      <td>٤ ساعات</td>\n",
       "      <td>تفاعل ‏٥٥٦‏ من الأشخاص مع هذا المنشور;تفاعل ‏٥...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>ازدياد بعدد العمليات و1500 ولادة طبيعية وقيصري...</td>\n",
       "      <td>٥ ساعات</td>\n",
       "      <td>تفاعل ‏٢٫١ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>#مؤسسة_أوغاريت _للسياحة_والسفر ✈️✈️\\nالأسعار ا...</td>\n",
       "      <td>٦ ساعات</td>\n",
       "      <td>تفاعل ‏٢٫٦ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>إعصار محطم للأعصاب يضرب ولاية أمريكية.. إليك ا...</td>\n",
       "      <td>٦ ساعات</td>\n",
       "      <td>تفاعل ‏٣٢٣‏ من الأشخاص مع هذا المنشور;تفاعل ‏٣...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>ارتفاع أسعار الموبايلات 300 %.. وبائعون: المور...</td>\n",
       "      <td>٦ ساعات</td>\n",
       "      <td>تفاعل ‏٣٫٢ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>لجنة مربي الدواجن تتوقع استمرار ارتفاع أسعار ا...</td>\n",
       "      <td>٧ ساعات</td>\n",
       "      <td>تفاعل ‏٢٫٨ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>أصدقاء الأمس أعداء اليوم.. قناة الأورينت تفتح ...</td>\n",
       "      <td>٧ ساعات</td>\n",
       "      <td>تفاعل ‏١٨٩‏ من الأشخاص مع هذا المنشور;تفاعل ‏١...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>وزير التجارة الداخلية وحماية المستهلك يقول : ط...</td>\n",
       "      <td>٨ ساعات</td>\n",
       "      <td>تفاعل ‏٤٫٢ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>الحب يقتل الطبيب.. من هو قاتل «كنان علي»؟\\n#كن...</td>\n",
       "      <td>٩ ساعات</td>\n",
       "      <td>تفاعل ‏١٫١ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>قريباً… زيادة جديدة في أسعار الإسمنت</td>\n",
       "      <td>٩ ساعات</td>\n",
       "      <td>تفاعل ‏٣٫٦ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>«آيفون 13» وتسريبات «غير سارة»</td>\n",
       "      <td>١١ ساعة</td>\n",
       "      <td>تفاعل ‏١٫٤ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>ارتفاع القهوة في «سوريا» سببه «البرازيل»!</td>\n",
       "      <td>١١ ساعة</td>\n",
       "      <td>تفاعل ‏٢٫٥ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>سوق العقارات.. 2 مليار دولار تدور في السوق ؟!</td>\n",
       "      <td>١٢ ساعة</td>\n",
       "      <td>تفاعل ‏٩١١‏ من الأشخاص مع هذا المنشور;تفاعل ‏٨...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>رفع أجور دقائق المكالمات الدولية 100% اعتباراً...</td>\n",
       "      <td>٢٣ ساعة</td>\n",
       "      <td>تفاعل ‏٥٫٧ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>أسعار الذهب في سوريا ليوم الاثنين .. الأونصة ت...</td>\n",
       "      <td>أمس الساعة ١٠:٠٠ م</td>\n",
       "      <td>تفاعل ‏٤ آلاف‏ من الأشخاص مع هذا المنشور;تفاعل...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>ازدياد الإصابات بكـ.ـورونا.. والصحة توضح: بسبب...</td>\n",
       "      <td>أمس الساعة ٩:٣٠ م</td>\n",
       "      <td>تفاعل ‏٩٫٣ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td>تحدي رهيب بين سليمان عاجي (أم تحسين) و فرح للص...</td>\n",
       "      <td>أمس الساعة ٩:٠٠ م</td>\n",
       "      <td>تفاعل ‏٢٫٧ ألف‏ من الأشخاص مع هذا المنشور;تفاع...</td>\n",
       "      <td>https://m.facebook.com/story.php?story_fbid=38...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_id                                               text  \\\n",
       "0           لتوليد الكهرباء.. دراسة للاستفادة من غاز الميث...   \n",
       "1           مجلس التعليم العالي: تسوية أوضاع الطلاب المستن...   \n",
       "2           لباس الرسمي الجديد للمنتخب السوري.. عمر خريبين...   \n",
       "3           مدرسة البشائر العربية الخاصة\\nالتسجيل مستمر لل...   \n",
       "4           تقدير كلفة تنظيف شاطئ بانياس من الفيول بـ3 ملا...   \n",
       "5           برأيكن بشو ممكن ترتبط السيارة بيوم السبت بالمت...   \n",
       "6           ازدياد بعدد العمليات و1500 ولادة طبيعية وقيصري...   \n",
       "7           #مؤسسة_أوغاريت _للسياحة_والسفر ✈️✈️\\nالأسعار ا...   \n",
       "8           إعصار محطم للأعصاب يضرب ولاية أمريكية.. إليك ا...   \n",
       "9           ارتفاع أسعار الموبايلات 300 %.. وبائعون: المور...   \n",
       "10          لجنة مربي الدواجن تتوقع استمرار ارتفاع أسعار ا...   \n",
       "11          أصدقاء الأمس أعداء اليوم.. قناة الأورينت تفتح ...   \n",
       "12          وزير التجارة الداخلية وحماية المستهلك يقول : ط...   \n",
       "13          الحب يقتل الطبيب.. من هو قاتل «كنان علي»؟\\n#كن...   \n",
       "14                       قريباً… زيادة جديدة في أسعار الإسمنت   \n",
       "15                             «آيفون 13» وتسريبات «غير سارة»   \n",
       "16                  ارتفاع القهوة في «سوريا» سببه «البرازيل»!   \n",
       "17              سوق العقارات.. 2 مليار دولار تدور في السوق ؟!   \n",
       "18          رفع أجور دقائق المكالمات الدولية 100% اعتباراً...   \n",
       "19          أسعار الذهب في سوريا ليوم الاثنين .. الأونصة ت...   \n",
       "20          ازدياد الإصابات بكـ.ـورونا.. والصحة توضح: بسبب...   \n",
       "21          تحدي رهيب بين سليمان عاجي (أم تحسين) و فرح للص...   \n",
       "\n",
       "                  time                                          reactions  \\\n",
       "0              ٧ دقائق                                                      \n",
       "1             ٣٨ دقيقة  تفاعل ‏٥١٩‏ من الأشخاص مع هذا المنشور;تفاعل ‏٥...   \n",
       "2           ساعة واحدة  تفاعل ‏٩٢٨‏ من الأشخاص مع هذا المنشور;تفاعل ‏٩...   \n",
       "3               ساعتان  تفاعل ‏٢٦٥‏ من الأشخاص مع هذا المنشور;تفاعل ‏٢...   \n",
       "4              ٣ ساعات  تفاعل ‏٢٫٤ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "5              ٤ ساعات  تفاعل ‏٥٥٦‏ من الأشخاص مع هذا المنشور;تفاعل ‏٥...   \n",
       "6              ٥ ساعات  تفاعل ‏٢٫١ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "7              ٦ ساعات  تفاعل ‏٢٫٦ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "8              ٦ ساعات  تفاعل ‏٣٢٣‏ من الأشخاص مع هذا المنشور;تفاعل ‏٣...   \n",
       "9              ٦ ساعات  تفاعل ‏٣٫٢ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "10             ٧ ساعات  تفاعل ‏٢٫٨ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "11             ٧ ساعات  تفاعل ‏١٨٩‏ من الأشخاص مع هذا المنشور;تفاعل ‏١...   \n",
       "12             ٨ ساعات  تفاعل ‏٤٫٢ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "13             ٩ ساعات  تفاعل ‏١٫١ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "14             ٩ ساعات  تفاعل ‏٣٫٦ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "15             ١١ ساعة  تفاعل ‏١٫٤ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "16             ١١ ساعة  تفاعل ‏٢٫٥ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "17             ١٢ ساعة  تفاعل ‏٩١١‏ من الأشخاص مع هذا المنشور;تفاعل ‏٨...   \n",
       "18             ٢٣ ساعة  تفاعل ‏٥٫٧ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "19  أمس الساعة ١٠:٠٠ م  تفاعل ‏٤ آلاف‏ من الأشخاص مع هذا المنشور;تفاعل...   \n",
       "20   أمس الساعة ٩:٣٠ م  تفاعل ‏٩٫٣ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "21   أمس الساعة ٩:٠٠ م  تفاعل ‏٢٫٧ ألف‏ من الأشخاص مع هذا المنشور;تفاع...   \n",
       "\n",
       "                                                  url  \n",
       "0   https://m.facebook.com/story.php?story_fbid=38...  \n",
       "1   https://m.facebook.com/story.php?story_fbid=38...  \n",
       "2   https://m.facebook.com/story.php?story_fbid=38...  \n",
       "3   https://m.facebook.com/story.php?story_fbid=38...  \n",
       "4   https://m.facebook.com/story.php?story_fbid=38...  \n",
       "5   https://m.facebook.com/story.php?story_fbid=38...  \n",
       "6   https://m.facebook.com/story.php?story_fbid=38...  \n",
       "7   https://m.facebook.com/story.php?story_fbid=38...  \n",
       "8   https://m.facebook.com/story.php?story_fbid=38...  \n",
       "9   https://m.facebook.com/story.php?story_fbid=38...  \n",
       "10  https://m.facebook.com/story.php?story_fbid=38...  \n",
       "11  https://m.facebook.com/story.php?story_fbid=38...  \n",
       "12  https://m.facebook.com/story.php?story_fbid=38...  \n",
       "13  https://m.facebook.com/story.php?story_fbid=38...  \n",
       "14  https://m.facebook.com/story.php?story_fbid=38...  \n",
       "15  https://m.facebook.com/story.php?story_fbid=38...  \n",
       "16  https://m.facebook.com/story.php?story_fbid=38...  \n",
       "17  https://m.facebook.com/story.php?story_fbid=38...  \n",
       "18  https://m.facebook.com/story.php?story_fbid=38...  \n",
       "19  https://m.facebook.com/story.php?story_fbid=38...  \n",
       "20  https://m.facebook.com/story.php?story_fbid=38...  \n",
       "21  https://m.facebook.com/story.php?story_fbid=38...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "<h3><left>- Saving.</left></h3>\n",
    "<h5><left>- save posts and pages.</left></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipline.save_data()\n",
    "pipline.save_pages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>- The End -</center></h2>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
